# Pairwise Experiments README

## Summary of Learnings

This notebook explores pairwise evaluation methods for comparing two different AI summarizer prompts using LLM-as-Judge evaluation, demonstrating how to set up comparative experiments with structured scoring and ranking preferences.

## Code Tweaks

Enhanced the judge prompt with additional evaluation criteria and implemented temperature variation (0.2 instead of default) for more consistent pairwise comparisons, plus added confidence scoring to the preference evaluation.