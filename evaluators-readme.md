# Evaluators README

## Summary of Learnings

This notebook explains how to create custom evaluators for LLM applications, including simple comparison evaluators and advanced LLM-as-Judge evaluators with structured output using Pydantic models for semantic similarity scoring.

## Code Tweaks

Implemented a hybrid scoring system combining rule-based and LLM evaluation methods, added contextual prompting with domain-specific instructions, and introduced confidence intervals for similarity scores to improve evaluation reliability.